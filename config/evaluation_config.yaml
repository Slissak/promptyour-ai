# Dynamic Model Evaluation Configuration
# This file controls all aspects of the model evaluation system
# Can be updated manually or programmatically

# ============================================================================
# EVALUATION SOURCES CONFIGURATION
# ============================================================================
evaluation_sources:
  huggingface_leaderboard:
    name: "HuggingFace Open LLM Leaderboard"
    url: "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
    type: "leaderboard"
    active: true
    update_frequency_hours: 6
    themes: ["academic", "research", "general"]
    scraping_config:
      selector: "table"
      score_columns: ["Average", "ARC", "HellaSwag", "MMLU", "TruthfulQA"]
      model_column: "Model"
      timeout: 30
    weight: 1.0
    reliability: 0.9

  chatbot_arena:
    name: "LMSYS Chatbot Arena"
    url: "https://chat.lmsys.org/"
    type: "arena"
    active: true
    update_frequency_hours: 2
    themes: ["creative", "general", "tutoring"]
    scraping_config:
      api_endpoint: "https://chat.lmsys.org/api/leaderboard"
      score_field: "rating"
      model_field: "model"
      timeout: 20
    weight: 1.2  # Higher weight for user-voted arena
    reliability: 0.95

  alpaca_eval:
    name: "AlpacaEval Benchmark"
    url: "https://tatsu-lab.github.io/alpaca_eval/"
    type: "benchmark"
    active: true
    update_frequency_hours: 24
    themes: ["tutoring", "academic", "general"]
    scraping_config:
      selector: ".leaderboard-table"
      score_columns: ["Win Rate"]
      model_column: "Model"
      timeout: 30
    weight: 0.8
    reliability: 0.85

  humaneval:
    name: "HumanEval Code Generation"
    url: "https://github.com/openai/human-eval"
    type: "benchmark"
    active: true
    update_frequency_hours: 48
    themes: ["coding"]
    scraping_config:
      type: "github_results"
      results_files: ["results.json", "leaderboard.md"]
      score_pattern: "pass@1"
      timeout: 45
    weight: 1.5  # Very important for coding tasks
    reliability: 0.9

  gsm8k:
    name: "GSM8K Math Reasoning"
    url: "https://github.com/openai/grade-school-math"
    type: "benchmark"
    active: true
    update_frequency_hours: 48
    themes: ["academic", "problem_solving", "tutoring"]
    scraping_config:
      type: "math_benchmark"
      score_pattern: "([\\w-]+)\\s*([0-9.]+)%"
      timeout: 30
    weight: 1.0
    reliability: 0.9

  mmlu:
    name: "MMLU Academic Benchmark"
    url: "https://github.com/hendrycks/test"
    type: "benchmark"
    active: true
    update_frequency_hours: 72
    themes: ["academic", "research"]
    scraping_config:
      type: "academic_benchmark"
      subjects: ["stem", "humanities", "social_sciences", "other"]
      timeout: 60
    weight: 1.3  # Important for academic tasks
    reliability: 0.95

  big_bench:
    name: "BIG-bench Evaluation"
    url: "https://github.com/google/BIG-bench"
    type: "benchmark"
    active: true
    update_frequency_hours: 72
    themes: ["academic", "research", "problem_solving"]
    scraping_config:
      type: "github_readme"
      score_pattern: "(\\w+[-\\w]*)\\s*:\\s*([0-9.]+)"
      timeout: 45
    weight: 0.9
    reliability: 0.8

  mtbench:
    name: "MT-Bench Conversation Quality"
    url: "https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge"
    type: "benchmark"
    active: true
    update_frequency_hours: 24
    themes: ["creative", "general", "tutoring"]
    scraping_config:
      type: "mt_bench"
      results_path: "mt_bench_scores.json"
      timeout: 30
    weight: 1.1
    reliability: 0.85

  # Easy to add new sources
  # new_benchmark:
  #   name: "New Evaluation Source"
  #   url: "https://example.com/leaderboard"
  #   type: "leaderboard"
  #   active: false  # Disabled until configured
  #   update_frequency_hours: 12
  #   themes: ["general"]
  #   scraping_config:
  #     selector: ".scores-table"
  #     score_columns: ["Score"]
  #     model_column: "Model"
  #   weight: 1.0
  #   reliability: 0.8

# ============================================================================
# THEME CONFIGURATION
# ============================================================================
themes:
  academic_help:
    display_name: "Academic Help"
    description: "Educational assistance for students and researchers"
    keywords: ["homework", "study", "research", "academic", "university", "school"]
    complexity_levels: ["beginner", "intermediate", "advanced", "academic"]
    active: true
    priority: 1.0

  creative_writing:
    display_name: "Creative Writing"
    description: "Creative content generation and storytelling"
    keywords: ["story", "creative", "writing", "fiction", "poetry", "narrative"]
    complexity_levels: ["intermediate", "advanced", "professional"]
    active: true
    priority: 1.0

  coding_programming:
    display_name: "Coding & Programming"
    description: "Software development and programming tasks"
    keywords: ["code", "programming", "development", "algorithm", "function", "debug"]
    complexity_levels: ["beginner", "intermediate", "advanced", "professional"]
    active: true
    priority: 1.2  # Higher priority for technical accuracy

  business_professional:
    display_name: "Business & Professional"
    description: "Business strategy, management, and professional tasks"
    keywords: ["business", "strategy", "management", "professional", "corporate", "finance"]
    complexity_levels: ["intermediate", "advanced", "professional"]
    active: true
    priority: 1.0

  personal_learning:
    display_name: "Personal Learning"
    description: "Learning new skills and personal development"
    keywords: ["learn", "hobby", "skill", "personal", "beginner", "tutorial"]
    complexity_levels: ["beginner", "intermediate"]
    active: true
    priority: 0.8

  research_analysis:
    display_name: "Research & Analysis"
    description: "In-depth research and analytical tasks"
    keywords: ["research", "analysis", "data", "study", "investigate", "analyze"]
    complexity_levels: ["advanced", "academic", "professional"]
    active: true
    priority: 1.1

  problem_solving:
    display_name: "Problem Solving"
    description: "Logic puzzles and problem-solving tasks"
    keywords: ["problem", "solve", "logic", "puzzle", "troubleshoot", "debug"]
    complexity_levels: ["beginner", "intermediate", "advanced"]
    active: true
    priority: 1.0

  tutoring_education:
    display_name: "Tutoring & Education"
    description: "Teaching and educational explanations"
    keywords: ["teach", "explain", "tutor", "education", "lesson", "guide"]
    complexity_levels: ["beginner", "intermediate", "advanced"]
    active: true
    priority: 0.9

  general_questions:
    display_name: "General Questions"
    description: "Everyday questions and general knowledge"
    keywords: ["what", "how", "why", "general", "question", "everyday"]
    complexity_levels: ["beginner", "intermediate"]
    active: true
    priority: 0.7

  # Easy to add new themes
  # scientific_research:
  #   display_name: "Scientific Research"
  #   description: "Advanced scientific research and methodology"
  #   keywords: ["scientific", "research", "experiment", "hypothesis", "methodology"]
  #   complexity_levels: ["academic", "professional"]
  #   active: false  # Can be enabled later
  #   priority: 1.2

# ============================================================================
# THEME-TO-SOURCE WEIGHT MAPPING
# ============================================================================
theme_source_weights:
  academic_help:
    mmlu: 0.35           # Strong academic benchmark
    gsm8k: 0.25          # Math reasoning
    big_bench: 0.20      # Broad reasoning
    huggingface_leaderboard: 0.15
    alpaca_eval: 0.05

  creative_writing:
    chatbot_arena: 0.40  # User preference for creativity
    mtbench: 0.30        # Conversation quality
    alpaca_eval: 0.20    # Instruction following
    huggingface_leaderboard: 0.10

  coding_programming:
    humaneval: 0.50      # Primary coding benchmark
    big_bench: 0.20      # Logic and reasoning
    huggingface_leaderboard: 0.15
    chatbot_arena: 0.10  # User preference
    mmlu: 0.05          # Technical knowledge

  business_professional:
    chatbot_arena: 0.30  # Professional communication
    alpaca_eval: 0.25    # Following business instructions
    mtbench: 0.20        # Multi-turn conversation
    huggingface_leaderboard: 0.15
    mmlu: 0.10          # Business knowledge

  personal_learning:
    alpaca_eval: 0.35    # Clear explanations
    chatbot_arena: 0.25  # User-friendly responses
    mtbench: 0.20        # Patient interaction
    gsm8k: 0.15          # Step-by-step reasoning
    huggingface_leaderboard: 0.05

  research_analysis:
    mmlu: 0.30          # Academic knowledge
    big_bench: 0.25     # Complex reasoning
    huggingface_leaderboard: 0.20
    alpaca_eval: 0.15   # Structured responses
    chatbot_arena: 0.10

  problem_solving:
    gsm8k: 0.35         # Mathematical problem solving
    big_bench: 0.30     # Logic puzzles
    humaneval: 0.20     # Algorithmic thinking
    mmlu: 0.10          # Knowledge application
    alpaca_eval: 0.05

  tutoring_education:
    alpaca_eval: 0.30   # Clear instructions
    gsm8k: 0.25         # Step-by-step explanations
    mtbench: 0.20       # Patient conversation
    chatbot_arena: 0.15 # User-friendly tone
    huggingface_leaderboard: 0.10

  general_questions:
    chatbot_arena: 0.35 # General user preference
    huggingface_leaderboard: 0.25
    alpaca_eval: 0.20   # Clear answers
    mtbench: 0.15       # Conversational
    mmlu: 0.05          # General knowledge

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model_config:
  # Cost tiers
  cost_tiers:
    budget: 0.01        # Max cost per 1K tokens for budget tier
    balanced: 0.05      # Max cost per 1K tokens for balanced tier
    premium: 1.0        # No limit for premium tier

  # Default model preferences by tier
  tier_preferences:
    budget: ["claude-3-haiku", "gpt-3.5-turbo", "gemini-pro"]
    balanced: ["claude-3-sonnet", "gpt-4", "claude-3-haiku"]
    premium: ["gpt-4", "claude-3-opus", "claude-3-sonnet"]

  # Model aliases for normalization
  model_aliases:
    "gpt-4-turbo": "gpt-4"
    "gpt-4-0125-preview": "gpt-4"
    "claude-3-opus-20240229": "claude-3-opus"
    "claude-3-sonnet-20240229": "claude-3-sonnet"
    "claude-3-haiku-20240307": "claude-3-haiku"
    "gemini-pro-1.0": "gemini-pro"

# ============================================================================
# SCORING CONFIGURATION
# ============================================================================
scoring:
  # How to combine static and dynamic scores
  score_combination:
    static_weight: 0.4    # Weight for static model configuration
    dynamic_weight: 0.6   # Weight for dynamic evaluation data

  # Confidence thresholds
  confidence:
    high_threshold: 0.8   # Above this = high confidence
    medium_threshold: 0.6 # Above this = medium confidence
    low_threshold: 0.4    # Below this = low confidence

  # Data freshness requirements
  data_freshness:
    very_fresh: 1         # Hours
    fresh: 6              # Hours
    acceptable: 24        # Hours
    stale: 72             # Hours
    very_stale: 168       # Hours (1 week)

# ============================================================================
# SCHEDULER CONFIGURATION
# ============================================================================
scheduler:
  # Scan intervals
  scan_intervals:
    full_scan_hours: 24
    incremental_hours: 6
    leaderboard_only_hours: 2

  # Retry configuration
  retry:
    max_retries: 3
    backoff_multiplier: 2
    initial_delay_seconds: 60

  # Concurrent scraping limits
  concurrency:
    max_concurrent_sources: 3
    request_delay_seconds: 2
    timeout_seconds: 60

# ============================================================================
# VALIDATION RULES
# ============================================================================
validation:
  # Minimum requirements for evaluation data
  min_sources_required: 2
  min_models_required: 3
  min_confidence_threshold: 0.3

  # Score validation
  score_ranges:
    min_score: 0
    max_score: 100
    
  # Model name validation
  model_name_patterns:
    - "^[a-zA-Z0-9-_]+$"  # Alphanumeric with hyphens and underscores
    - "^.{1,50}$"         # Max 50 characters

# ============================================================================
# FEATURE FLAGS
# ============================================================================
features:
  enable_web_scraping: true
  enable_caching: true
  enable_scheduler: true
  enable_api_endpoints: true
  enable_theme_learning: true     # Learn theme preferences from user feedback
  enable_cost_optimization: true
  enable_fallback_models: true
  debug_mode: false