# Terminal Chat Interface Guide

A beautiful, interactive terminal-based chat interface for testing the PromptYour.AI backend.

## ğŸš€ Quick Start

### 1. Start the Backend

```bash
# Option 1: Using the environment setup
PYTHONPATH=src/backend python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# Option 2: If you have make configured
make run-backend
```

### 2. Start the Terminal Chat

```bash
# Option 1: Simple start
python terminal_chat.py

# Option 2: Using the convenience script
./run_chat.sh

# Option 3: With custom API URL
python terminal_chat.py --api http://localhost:8000

# Option 4: HTTP-only mode (no WebSocket)
python terminal_chat.py --http-only
```

## ğŸ“¦ Dependencies

The chat interface requires these packages:
```bash
pip install httpx websockets rich
```

These will be automatically installed when you run `./run_chat.sh`.

## ğŸ¯ Features

### âœ¨ **Interactive Chat**
- Real-time responses via WebSocket
- Beautiful terminal UI with colors and formatting
- Automatic fallback to HTTP if WebSocket fails

### ğŸ¨ **Rich Interface**
- Colorful, formatted output
- Progress indicators during processing
- Markdown rendering for AI responses
- Interactive prompts and menus

### ğŸ”§ **Provider Integration**
- Automatic detection of LM Studio and OpenRouter
- Real-time provider status display
- Shows which model and provider was used
- Cost tracking per message

### ğŸ“Š **Session Management**
- Conversation history within session
- Session statistics (cost, response times, providers used)
- Message rating system

## ğŸ® Usage

### Basic Chat
Just type your question and press Enter:
```
> What is Python?
```

### Theme Selection
For better results, you can specify themes:
1. Academic Help
2. Creative Writing  
3. Coding/Programming
4. Business/Professional
5. Personal Learning
6. Research/Analysis
7. Problem Solving
8. Tutoring/Education
9. General Questions

### Commands
- `/help` - Show help information
- `/status` - Check backend and provider status
- `/stats` - Show session statistics
- `/clear` - Clear screen
- `/quit` or `/exit` - Exit chat

### Example Session
```
> Hello, can you help me with Python?

ğŸ¯ Customize theme and context for better results? [y/N]: y

ğŸ’­ Your question: Hello, can you help me with Python?

ğŸ“‹ Available themes:
   1. Academic Help
   2. Creative Writing
   3. Coding Programming    â† Choose this for Python questions
   ...

ğŸ¯ Choose theme (1-9, or press Enter for general): 3

ğŸ“ Additional context (optional): I'm a beginner learning basic syntax

[Processing with spinner animation...]

ğŸ¤– AI Response:
[Formatted markdown response with syntax highlighting]

ğŸ¤– Model: claude-3-haiku (via qwen/qwen3-4b-2507)
ğŸ  Provider: lm_studio
ğŸ’° Cost: $0.000000
â±ï¸  Response Time: 2341ms

â­ Rate this response (1-5, or press Enter to skip): 5
âœ… Rating submitted: 5/5
```

## ğŸ”Œ Backend Connection

The chat connects to your backend at `http://localhost:8000` by default.

**Status Indicators:**
- âœ… Green: Provider healthy and ready
- âŒ Red: Provider unavailable or error
- âš ï¸ Yellow: Provider partially available

**Provider Information:**
- **LM Studio**: Shows loaded model if available
- **OpenRouter**: Shows API key status

## ğŸ¨ Interface Features

### **Welcome Screen**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ¤– PromptYour.AI Terminal Chat               â•‘
â•‘                                                           â•‘
â•‘  Enhanced AI responses through intelligent model routing  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”Œ Provider Status
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Provider    â”ƒ Status  â”ƒ Details                      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Lm Studio   â”‚ healthy â”‚ Model: qwen/qwen3-4b-2507    â”‚
â”‚ Openrouter  â”‚ healthy â”‚ API Key: âœ…                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Response Display**
- **Metadata Table**: Model, provider, cost, timing
- **Formatted Response**: Markdown with syntax highlighting
- **Interactive Rating**: 1-5 star rating system

### **Session Statistics**
```
ğŸ“Š Session Statistics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value                         â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Total Messages   â”‚ 5                             â”‚
â”‚ Total Cost       â”‚ $0.000000                     â”‚
â”‚ Avg Response Timeâ”‚ 2841ms                        â”‚
â”‚ Most Used Providerâ”‚ lm_studio                     â”‚
â”‚ Most Used Model  â”‚ qwen/qwen3-4b-2507            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› Troubleshooting

### Backend Not Running
```
âŒ Cannot connect to backend. Make sure it's running!
```
**Solution**: Start the backend first:
```bash
PYTHONPATH=src/backend python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Missing Dependencies
```
Missing required packages. Please install them:
pip install httpx websockets rich
```
**Solution**: Install the required packages or use `./run_chat.sh`

### WebSocket Connection Failed
The chat will automatically fall back to HTTP mode if WebSocket fails.

### LM Studio Not Detected
- Make sure LM Studio is running
- Check that a model is loaded
- Verify the local server is started on port 1234

## ğŸ¯ Pro Tips

### **For Best Results:**
1. **Use specific themes** - Choose the most relevant theme for your question
2. **Add context** - Provide background information when helpful  
3. **Try different models** - Switch models in LM Studio to see different responses
4. **Rate responses** - Help improve the system by rating answers

### **Performance:**
- **Local models** (LM Studio) are free but may be slower
- **Cloud models** (OpenRouter) are faster but cost money
- The system automatically chooses the best available option

### **Advanced Usage:**
```bash
# Use different API endpoint
python terminal_chat.py --api http://192.168.1.100:8000

# Force HTTP mode for debugging
python terminal_chat.py --http-only

# Run with verbose output
python terminal_chat.py -v
```

## ğŸ”§ Configuration

The chat interface reads the same configuration as the backend:
- **LM Studio URL**: Automatically detected at `http://localhost:1234`
- **Provider Preferences**: Uses backend's `PREFER_LOCAL_LLM` setting
- **Model Selection**: Based on theme and backend's model selector

## ğŸ“ Notes

- **Conversation Persistence**: Currently within session only
- **History**: Each session generates a unique conversation ID
- **Ratings**: Collected but require backend storage implementation
- **Offline Mode**: Works with LM Studio when internet unavailable

The terminal chat provides a full-featured way to test and interact with your PromptYour.AI backend, showcasing all the intelligent routing, model selection, and provider integration features in a beautiful, user-friendly interface.